{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport string\nfrom nltk.corpus import stopwords","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-12T07:07:01.68127Z","iopub.execute_input":"2022-09-12T07:07:01.682138Z","iopub.status.idle":"2022-09-12T07:07:04.52407Z","shell.execute_reply.started":"2022-09-12T07:07:01.682101Z","shell.execute_reply":"2022-09-12T07:07:04.523066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\ntest = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\nss = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:04.525944Z","iopub.execute_input":"2022-09-12T07:07:04.527576Z","iopub.status.idle":"2022-09-12T07:07:04.720836Z","shell.execute_reply.started":"2022-09-12T07:07:04.527537Z","shell.execute_reply":"2022-09-12T07:07:04.719871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:04.723064Z","iopub.execute_input":"2022-09-12T07:07:04.723442Z","iopub.status.idle":"2022-09-12T07:07:04.744632Z","shell.execute_reply.started":"2022-09-12T07:07:04.723407Z","shell.execute_reply":"2022-09-12T07:07:04.743722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Check distributions","metadata":{}},{"cell_type":"code","source":"train.cohesion.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:04.747334Z","iopub.execute_input":"2022-09-12T07:07:04.747968Z","iopub.status.idle":"2022-09-12T07:07:04.995201Z","shell.execute_reply.started":"2022-09-12T07:07:04.747933Z","shell.execute_reply":"2022-09-12T07:07:04.994341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.syntax.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:04.996424Z","iopub.execute_input":"2022-09-12T07:07:04.997673Z","iopub.status.idle":"2022-09-12T07:07:05.229111Z","shell.execute_reply.started":"2022-09-12T07:07:04.997635Z","shell.execute_reply":"2022-09-12T07:07:05.228218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.vocabulary.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:05.230266Z","iopub.execute_input":"2022-09-12T07:07:05.231536Z","iopub.status.idle":"2022-09-12T07:07:05.464842Z","shell.execute_reply.started":"2022-09-12T07:07:05.231499Z","shell.execute_reply":"2022-09-12T07:07:05.463759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.phraseology.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:05.466414Z","iopub.execute_input":"2022-09-12T07:07:05.467151Z","iopub.status.idle":"2022-09-12T07:07:05.761714Z","shell.execute_reply.started":"2022-09-12T07:07:05.467114Z","shell.execute_reply":"2022-09-12T07:07:05.760808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.grammar.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:05.762972Z","iopub.execute_input":"2022-09-12T07:07:05.76386Z","iopub.status.idle":"2022-09-12T07:07:05.988154Z","shell.execute_reply.started":"2022-09-12T07:07:05.763822Z","shell.execute_reply":"2022-09-12T07:07:05.987276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.conventions.hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:05.989375Z","iopub.execute_input":"2022-09-12T07:07:05.990247Z","iopub.status.idle":"2022-09-12T07:07:06.261232Z","shell.execute_reply.started":"2022-09-12T07:07:05.99021Z","shell.execute_reply":"2022-09-12T07:07:06.260355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check length of content\ndef get_length_of_text(x):\n    return len(x)\n\nprint(f'Average length: {train.full_text.apply(lambda x: get_length_of_text(x)).mean():0.2f}')\nprint(f'Std length: {train.full_text.apply(lambda x: get_length_of_text(x)).std():0.2f}')\nprint(f'Min length: {train.full_text.apply(lambda x: get_length_of_text(x)).min():0.2f}')\nprint(f'Max length: {train.full_text.apply(lambda x: get_length_of_text(x)).max():0.2f}')","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:06.264092Z","iopub.execute_input":"2022-09-12T07:07:06.268551Z","iopub.status.idle":"2022-09-12T07:07:06.295088Z","shell.execute_reply.started":"2022-09-12T07:07:06.268514Z","shell.execute_reply":"2022-09-12T07:07:06.294297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.full_text.apply(lambda x: get_length_of_text(x)).hist();","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:06.298923Z","iopub.execute_input":"2022-09-12T07:07:06.301555Z","iopub.status.idle":"2022-09-12T07:07:06.563839Z","shell.execute_reply.started":"2022-09-12T07:07:06.301515Z","shell.execute_reply":"2022-09-12T07:07:06.563006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First question: why is there no value between 2.5 and 3 for any of the variables?","metadata":{}},{"cell_type":"markdown","source":"## Check nans","metadata":{}},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:06.567832Z","iopub.execute_input":"2022-09-12T07:07:06.570022Z","iopub.status.idle":"2022-09-12T07:07:06.584146Z","shell.execute_reply.started":"2022-09-12T07:07:06.569985Z","shell.execute_reply":"2022-09-12T07:07:06.582614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check correlations","metadata":{}},{"cell_type":"code","source":"colormap = sns.color_palette(\"Blues\")\nsns.heatmap(train.corr(), annot=True, cmap=colormap);","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:06.588836Z","iopub.execute_input":"2022-09-12T07:07:06.590969Z","iopub.status.idle":"2022-09-12T07:07:07.093978Z","shell.execute_reply.started":"2022-09-12T07:07:06.590934Z","shell.execute_reply":"2022-09-12T07:07:07.093049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All variables are kinda correlated with each other","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"The idea here is to predict all variables. \n\nTwo approaches:\n- Multioutput regression\n- Single output regression x6","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:07.225182Z","iopub.execute_input":"2022-09-12T07:07:07.225844Z","iopub.status.idle":"2022-09-12T07:07:07.240228Z","shell.execute_reply.started":"2022-09-12T07:07:07.225813Z","shell.execute_reply":"2022-09-12T07:07:07.239283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.copy()\n# we are going to iterate through each target variable\ntarget_vars = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:07.580864Z","iopub.execute_input":"2022-09-12T07:07:07.581194Z","iopub.status.idle":"2022-09-12T07:07:07.588402Z","shell.execute_reply.started":"2022-09-12T07:07:07.581166Z","shell.execute_reply":"2022-09-12T07:07:07.587456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to extract the vectors from the text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(smooth_idf=True, sublinear_tf=True) # this should be tuned in the future\nvectorizer.fit(raw_documents=train.full_text)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:07:08.104952Z","iopub.execute_input":"2022-09-12T07:07:08.105779Z","iopub.status.idle":"2022-09-12T07:07:09.049746Z","shell.execute_reply.started":"2022-09-12T07:07:08.105732Z","shell.execute_reply":"2022-09-12T07:07:09.048811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_vectors(x):\n    vecs = vectorizer.transform(x)\n    return vecs.toarray().flatten()\n\n# extract_vectors([train.iloc[0].full_text])\ndf['vecs'] = train.full_text.progress_apply(lambda x: extract_vectors([x]))\n\ndef syllable_count(word):\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count\n\n\ndef flesch_kincaid_score(x):\n    '206.835 - 1.015 × (total words ÷ total sentences) - 84.6 × (total syllables ÷ total words).'\n    total_words = len(x.split())\n    total_sentences = (len(x.split('.')))\n    syllables = sum([syllable_count(w) for w in x.split()])\n    return 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (syllables / total_words)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:15.091058Z","iopub.execute_input":"2022-09-12T07:11:15.091443Z","iopub.status.idle":"2022-09-12T07:11:20.568131Z","shell.execute_reply.started":"2022-09-12T07:11:15.09141Z","shell.execute_reply":"2022-09-12T07:11:20.566855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract features from text\nstop_words = stopwords.words('english')\ndf['char_count'] = df['full_text'].progress_apply(len)\ndf['word_count'] = df['full_text'].progress_apply(lambda x: len(x.split()))\ndf['word_density'] = df['char_count'] / (df['word_count'] + 1)\ndf['punctuation_count'] = df['full_text'].progress_apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \ndf['title_word_count'] = df['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\ndf['upper_case_word_count'] = df['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\ndf['stopword_count'] = df['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\ndf['flesch_kincaid_score'] = df['full_text'].progress_apply(lambda x: flesch_kincaid_score(x))","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:30.407206Z","iopub.execute_input":"2022-09-12T07:11:30.40775Z","iopub.status.idle":"2022-09-12T07:11:37.854122Z","shell.execute_reply.started":"2022-09-12T07:11:30.407713Z","shell.execute_reply":"2022-09-12T07:11:37.853141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng_features = df.drop(['text_id', 'full_text', 'cohesion','syntax', 'vocabulary',\n       'phraseology', 'grammar', 'conventions', 'vecs'], axis=1).columns","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:46.973102Z","iopub.execute_input":"2022-09-12T07:11:46.974031Z","iopub.status.idle":"2022-09-12T07:11:46.980111Z","shell.execute_reply.started":"2022-09-12T07:11:46.973992Z","shell.execute_reply":"2022-09-12T07:11:46.979035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng_features","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:47.17784Z","iopub.execute_input":"2022-09-12T07:11:47.178544Z","iopub.status.idle":"2022-09-12T07:11:47.18576Z","shell.execute_reply.started":"2022-09-12T07:11:47.178505Z","shell.execute_reply":"2022-09-12T07:11:47.184691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_set = []\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    vecs = row['vecs']\n    vals = row[eng_features].astype(float)\n    features = np.hstack([vecs, vals]).flatten()\n    feature_set.append(features)\nX = np.array(feature_set)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:48.294197Z","iopub.execute_input":"2022-09-12T07:11:48.294889Z","iopub.status.idle":"2022-09-12T07:11:50.76859Z","shell.execute_reply.started":"2022-09-12T07:11:48.294853Z","shell.execute_reply":"2022-09-12T07:11:50.767587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohesion_train = df.cohesion.values\nsyntax_train = df.syntax.values\nvocabulary_train = df.vocabulary.values\nphraseology_train = df.phraseology.values\ngrammar_train = df.grammar.values\nconventions_train = df.conventions.values","metadata":{"execution":{"iopub.status.busy":"2022-09-12T07:11:50.770633Z","iopub.execute_input":"2022-09-12T07:11:50.771013Z","iopub.status.idle":"2022-09-12T07:11:50.777382Z","shell.execute_reply.started":"2022-09-12T07:11:50.770977Z","shell.execute_reply":"2022-09-12T07:11:50.776357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter optimization with Optuna","metadata":{}},{"cell_type":"code","source":"'''\nimport optuna\nfrom lightgbm.callback import log_evaluation, early_stopping\n\ndef objective(trial, data=X, target=conventions_train):\n    \n    train_x, test_x, train_y, test_y = model_selection.train_test_split(data, target, test_size=0.3, random_state=42)\n    param = {\n        'metric': 'rmse', \n        'random_state': 42,\n        'n_estimators': trial.suggest_int('n_estimators', 10, 500),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.001, 0.01, 0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.2, 0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.004, 0.008, 0.01, 0.02, 0.05, .1, 0.2, 0.5]),\n        'max_depth': trial.suggest_categorical('max_depth', [10, 20,100, 150]),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(train_x, train_y, eval_set=[(test_x, test_y)], callbacks=[log_evaluation(period=0)])\n    \n    preds = model.predict(test_x)\n    \n    rmse = np.sqrt(metrics.mean_squared_error(test_y, preds))\n    \n    return rmse\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:04:59.995333Z","iopub.execute_input":"2022-09-12T09:04:59.995919Z","iopub.status.idle":"2022-09-12T09:05:00.006741Z","shell.execute_reply.started":"2022-09-12T09:04:59.995878Z","shell.execute_reply":"2022-09-12T09:05:00.005771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\noptuna.logging.set_verbosity(optuna.logging.INFO)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint(f'Number of finished trials: {len(study.trials)}')\nprint(f'Best trial: {study.best_trial.params}')\nprint(f'Best score: {study.best_value}')\n'''","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:05:00.270249Z","iopub.execute_input":"2022-09-12T09:05:00.270941Z","iopub.status.idle":"2022-09-12T09:11:20.648138Z","shell.execute_reply.started":"2022-09-12T09:05:00.270907Z","shell.execute_reply":"2022-09-12T09:11:20.647107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohesion_best_params = {'n_estimators': 420, 'reg_alpha': 0.13919864437901744, 'reg_lambda': 0.5069801040693652, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.02, 'max_depth': 100, 'min_child_samples': 99}\nsyntax_best_params = {'n_estimators': 500, 'reg_alpha': 0.25908918503224804, 'reg_lambda': 0.004271708198460402, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.02, 'max_depth': 150, 'min_child_samples': 61}\nvocabulary_best_params = {'n_estimators': 428, 'reg_alpha': 0.0010889416899550251, 'reg_lambda': 0.20016253704202466, 'colsample_bytree': 0.4, 'subsample': 0.6, 'learning_rate': 0.02, 'max_depth': 10, 'min_child_samples': 19}\nphraseology_best_params = {'n_estimators': 426, 'reg_alpha': 0.09286405380355575, 'reg_lambda': 0.011669618789040185, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.02, 'max_depth': 100, 'min_child_samples': 1}\ngrammar_best_params = {'n_estimators': 410, 'reg_alpha': 0.002402918599536554, 'reg_lambda': 0.011524180731876684, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.02, 'max_depth': 10, 'min_child_samples': 40}\nconventions_best_params = {'n_estimators': 468, 'reg_alpha': 0.057858250513145686, 'reg_lambda': 4.601288604571916, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.05, 'max_depth': 150, 'min_child_samples': 34}","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:12:04.053824Z","iopub.execute_input":"2022-09-12T09:12:04.054482Z","iopub.status.idle":"2022-09-12T09:12:04.062855Z","shell.execute_reply.started":"2022-09-12T09:12:04.054445Z","shell.execute_reply":"2022-09-12T09:12:04.061493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_performances_container = []\nval_performances_container = []","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:12:04.478946Z","iopub.execute_input":"2022-09-12T09:12:04.479881Z","iopub.status.idle":"2022-09-12T09:12:04.484293Z","shell.execute_reply.started":"2022-09-12T09:12:04.479847Z","shell.execute_reply":"2022-09-12T09:12:04.483357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_strategy = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n\nperformances = {}\n\nfor var in [(\"cohesion\", cohesion_train, cohesion_best_params), (\"syntax\", syntax_train, syntax_best_params), (\"vocabulary\", vocabulary_train, vocabulary_best_params), \n            (\"phraseology\", phraseology_train, phraseology_best_params), (\"grammar\", grammar_train, grammar_best_params), (\"conventions\", conventions_train, conventions_best_params)]:\n    print(f'Validating on {var[0]}')\n    fold = 0\n    train_scores = []\n    val_scores = []\n    \n    for train_idx, val_idx in cv_strategy.split(X, var[1]):\n\n        # train\n        X_train = X[train_idx]\n        y_train = var[1][train_idx]\n\n        # validation\n        X_val = X[val_idx]\n        y_val = var[1][val_idx]\n\n        # training\n        model = LGBMRegressor(**var[2])\n        model.fit(X_train, y_train)\n\n        # predicting\n        train_preds = model.predict(X_train)\n        val_preds = model.predict(X_val)\n\n        # storing scores\n        train_score = np.sqrt(metrics.mean_squared_error(y_train, train_preds))\n        val_score = np.sqrt(metrics.mean_squared_error(y_val, val_preds))\n\n        train_scores.append(train_score)\n        val_scores.append(val_score)\n\n        print(f\"Fold {fold} ==> Train accuracy: {train_score:0.4f} | Validation accuracy: {val_score:0.4f}\")\n        fold += 1\n\n    training_performance = np.mean(train_scores)\n    val_performance = np.mean(val_scores)\n    \n    training_performances_container.append(training_performance)\n    val_performances_container.append(val_performance)\n    print(f\"END. Average training performance: {training_performance:0.4f} | Average validation performance: {val_performance:0.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:12:05.246222Z","iopub.execute_input":"2022-09-12T09:12:05.246977Z","iopub.status.idle":"2022-09-12T09:19:48.42398Z","shell.execute_reply.started":"2022-09-12T09:12:05.246938Z","shell.execute_reply":"2022-09-12T09:19:48.423126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohesion_model = LGBMRegressor(**cohesion_best_params)\nsyntax_model = LGBMRegressor(**syntax_best_params)\nvocabulary_model = LGBMRegressor(**vocabulary_best_params)\nphraseology_model = LGBMRegressor(**phraseology_best_params)\ngrammar_model = LGBMRegressor(**grammar_best_params)\nconventions_model = LGBMRegressor(**conventions_best_params)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:19:48.427621Z","iopub.execute_input":"2022-09-12T09:19:48.429556Z","iopub.status.idle":"2022-09-12T09:19:48.435846Z","shell.execute_reply.started":"2022-09-12T09:19:48.429521Z","shell.execute_reply":"2022-09-12T09:19:48.434819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Fitting Cohesion Model')\ncohesion_model.fit(X, cohesion_train)\nprint(f'Fitting Syntax Model')\nsyntax_model.fit(X, syntax_train)\nprint(f'Fitting Vocabulary Model')\nvocabulary_model.fit(X, vocabulary_train)\nprint(f'Fitting Phraseology Model')\nphraseology_model.fit(X, phraseology_train)\nprint(f'Fitting Grammar Model')\ngrammar_model.fit(X, grammar_train)\nprint(f'Fitting Conventions Model')\nconventions_model.fit(X, conventions_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:19:48.43845Z","iopub.execute_input":"2022-09-12T09:19:48.438778Z","iopub.status.idle":"2022-09-12T09:21:34.140712Z","shell.execute_reply.started":"2022-09-12T09:19:48.438753Z","shell.execute_reply":"2022-09-12T09:21:34.13991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_test = test.copy()\n_test['vecs'] = _test.full_text.apply(lambda x: extract_vectors([x]))\n_test['char_count'] = _test['full_text'].progress_apply(len)\n_test['word_count'] = _test['full_text'].progress_apply(lambda x: len(x.split()))\n_test['word_density'] = _test['char_count'] / (_test['word_count']+1)\n_test['punctuation_count'] = _test['full_text'].progress_apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n_test['title_word_count'] = _test['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n_test['upper_case_word_count'] = _test['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n_test['stopword_count'] = _test['full_text'].progress_apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n_test['flesch_kincaid_score'] = _test['full_text'].progress_apply(lambda x: flesch_kincaid_score(x))","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:21:34.144857Z","iopub.execute_input":"2022-09-12T09:21:34.145554Z","iopub.status.idle":"2022-09-12T09:21:34.207561Z","shell.execute_reply.started":"2022-09-12T09:21:34.145516Z","shell.execute_reply":"2022-09-12T09:21:34.206444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feature_set = []\nfor i, row in tqdm(_test.iterrows(), total=len(_test)):\n    vecs = row['vecs']\n    vals = row[eng_features].astype(float)\n    features = np.hstack([vecs, vals]).flatten()\n    test_feature_set.append(features)\nX_test = np.array(test_feature_set)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:21:34.209195Z","iopub.execute_input":"2022-09-12T09:21:34.209878Z","iopub.status.idle":"2022-09-12T09:21:34.223091Z","shell.execute_reply.started":"2022-09-12T09:21:34.209841Z","shell.execute_reply":"2022-09-12T09:21:34.221942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohesion_predictions = cohesion_model.predict(X_test)\nsyntax_predictions = syntax_model.predict(X_test)\nvocabulary_predictions = vocabulary_model.predict(X_test)\nphraseology_predictions = phraseology_model.predict(X_test)\ngrammar_predictions = grammar_model.predict(X_test)\nconventions_predictions = conventions_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:21:34.224875Z","iopub.execute_input":"2022-09-12T09:21:34.225416Z","iopub.status.idle":"2022-09-12T09:21:34.238283Z","shell.execute_reply.started":"2022-09-12T09:21:34.225374Z","shell.execute_reply":"2022-09-12T09:21:34.236153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = ss.copy()\nsubmission.cohesion = cohesion_predictions\nsubmission.syntax = syntax_predictions\nsubmission.vocabulary = vocabulary_predictions\nsubmission.phraseology = phraseology_predictions\nsubmission.grammar = grammar_predictions\nsubmission.conventions = conventions_predictions","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:21:34.239796Z","iopub.execute_input":"2022-09-12T09:21:34.240117Z","iopub.status.idle":"2022-09-12T09:21:34.252045Z","shell.execute_reply.started":"2022-09-12T09:21:34.240088Z","shell.execute_reply":"2022-09-12T09:21:34.250851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-12T09:21:34.253492Z","iopub.execute_input":"2022-09-12T09:21:34.253812Z","iopub.status.idle":"2022-09-12T09:21:34.261442Z","shell.execute_reply.started":"2022-09-12T09:21:34.253782Z","shell.execute_reply":"2022-09-12T09:21:34.260318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}